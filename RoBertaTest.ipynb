{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "796b1197-cd3b-4a16-9e87-5d9f2dded106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "import onnx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.onnx import TrainingMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f5ff032-d970-41ff-ad13-595c343764d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aa2628-c5b2-4d79-b4cc-cead15e22c7d",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2a4383-a7dd-48be-ada9-7b5dec4654e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"joeddav/xlm-roberta-large-xnli\"\n",
    "classifier = pipeline(\"zero-shot-classification\", model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b92dcde2-f3a8-4938-9ac9-771c93157d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = \"Manchester United\"\n",
    "# we can specify candidate labels in Russian or any other language above:\n",
    "candidate_labels = [\"football team\", \"city\", \"England\", \"Masha\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "703e6b04-08af-453d-98de-f7db4aa36c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Manchester United',\n",
       " 'labels': ['football team', 'city', 'England', 'Masha'],\n",
       " 'scores': [0.9946267604827881,\n",
       "  0.5824565887451172,\n",
       "  0.4575895667076111,\n",
       "  0.0036103464663028717]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(sequence_to_classify, candidate_labels, multi_label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39518b8d-2ab1-421d-88cc-616b9bad0d93",
   "metadata": {},
   "source": [
    "## Parts of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d2e5046-a8a9-4df9-ab2a-3f101d2ed6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"joeddav/xlm-roberta-large-xnli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3768d426-b09b-4589-a857-492bdecdfc90",
   "metadata": {},
   "source": [
    "### Get scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "263de525-a1fb-4956-bb66-4f43c8256730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_folder = \"/root/data/\"\n",
    "data = pd.read_csv(data_folder + \"markup_w_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce2a759b-6186-4e89-8c5b-b6d4199649c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:13<00:00, 35.88it/s]\n"
     ]
    }
   ],
   "source": [
    "data_input = []\n",
    "data_attention = []\n",
    "data_scores = []\n",
    "\n",
    "for i in tqdm(range(500)):\n",
    "\n",
    "    candidates = data.loc[i, [\"level3_name\", \"level4_name\", \"type\"]].values\n",
    "    name = data.loc[i, \"name\"]\n",
    "    hypothesis_template = \"Это товар из категории {}.\"\n",
    "\n",
    "    sequence_pairs = []\n",
    "    for candidate_label in candidates:\n",
    "        sequence_pairs.extend([[name, hypothesis_template.format(candidate_label)]])\n",
    "\n",
    "    x = tokenizer(\n",
    "        sequence_pairs,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=\"only_first\",\n",
    "        pad_to_multiple_of=128,\n",
    "        max_length=128,\n",
    "    )\n",
    "    data_input.append(x[\"input_ids\"].numpy())\n",
    "    data_attention.append(x[\"attention_mask\"].numpy())\n",
    "\n",
    "    output = model(x[\"input_ids\"].to(device), x[\"attention_mask\"].to(device))\n",
    "\n",
    "    entail_contradiction_logits = output.logits[:, [0, 2]]\n",
    "    probs = entail_contradiction_logits.softmax(dim=1)\n",
    "    prob_label_is_true = probs[:, 1].detach().cpu().numpy()\n",
    "\n",
    "    example = {\n",
    "        key: value\n",
    "        for key, value in zip(\n",
    "            [\"cat3_score\", \"cat4_score\", \"type_score\"], prob_label_is_true\n",
    "        )\n",
    "    }\n",
    "    data_scores.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19faaba5-fb3b-488d-b7b5-f45ff415b575",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_folder + \"input.npy\", \"wb\") as f:\n",
    "    np.save(f, np.stack(data_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e87338ab-3dc6-4718-86d3-40f6fc0e53db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_folder + \"attention.npy\", \"wb\") as f:\n",
    "    np.save(f, np.stack(data_attention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b51489d-8244-4204-aae0-5887f50896b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data_scores).to_csv(data_folder + \"scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f138c1-d03a-408a-998c-3ae360ea61e4",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ef82561-7105-4b70-8a3b-62cedb95ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_template = \"This example is {}.\"\n",
    "\n",
    "\n",
    "sequence_pairs = []\n",
    "for candidate_label in candidate_labels:\n",
    "    sequence_pairs.extend(\n",
    "        [[sequence_to_classify, hypothesis_template.format(candidate_label)]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a44ed0c7-ca58-44af-98b4-1f215a6dcbc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Manchester United', 'This example is football team.'],\n",
       " ['Manchester United', 'This example is city.'],\n",
       " ['Manchester United', 'This example is England.'],\n",
       " ['Manchester United', 'This example is Masha.']]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "484ad818-dd37-44c0-9bbb-2f5eca49dcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ce65bf9b-c9a9-4266-87ac-c795df5b0d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer(\n",
    "    sequence_pairs,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=\"only_first\",\n",
    "    pad_to_multiple_of=15,\n",
    "    max_length=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6c9aef09-3d60-43af-877b-96c1f9422272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 15])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9c5da3b5-fa2b-457a-a845-67c3cadc05f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     0,  30749,  14098,      2,      2,   3293,  27781,     83, 101740,\n",
       "           7175,      5,      2,      1,      1,      1],\n",
       "        [     0,  30749,  14098,      2,      2,   3293,  27781,     83,  26349,\n",
       "              5,      2,      1,      1,      1,      1],\n",
       "        [     0,  30749,  14098,      2,      2,   3293,  27781,     83,  30715,\n",
       "              5,      2,      1,      1,      1,      1],\n",
       "        [     0,  30749,  14098,      2,      2,   3293,  27781,     83, 122203,\n",
       "              5,      2,      1,      1,      1,      1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "593df4b5-ea74-495f-866d-ff5a1eb14c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Manchester United</s></s> This example is Masha.</s><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(x[\"input_ids\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e76a507d-b56d-4096-8c0e-2d710e928383",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(x[\"input_ids\"].to(device), x[\"attention_mask\"].to(device))\n",
    "\n",
    "entail_contradiction_logits = output.logits[:, [0, 2]]\n",
    "probs = entail_contradiction_logits.softmax(dim=1)\n",
    "prob_label_is_true = probs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "dcced8d6-faf1-4366-b7b9-0db5863b4d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0021, 0.6138, 0.3842],\n",
       "        [0.1256, 0.6962, 0.1782],\n",
       "        [0.2491, 0.5425, 0.2084],\n",
       "        [0.9781, 0.0184, 0.0036]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "dd42a605-5377-46b4-ac00-6580772a7bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['football team', 'city', 'England', 'Masha']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "42d64d35-1492-4d09-be86-aeddae6214cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9946, 0.5850, 0.4567, 0.0036], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_label_is_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d5fa1b-9276-4890-9000-d5fbf7fb5ca4",
   "metadata": {},
   "source": [
    "## ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4efb0e7-132a-4996-a676-a703bd74be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_onnx(net, x, device):\n",
    "\n",
    "    # export the model to ONNX\n",
    "    input_ids = x[\"input_ids\"].type(torch.int32).to(device)\n",
    "    attention_mask = x[\"attention_mask\"].type(torch.int32).to(device)\n",
    "    net = net.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.onnx.export(\n",
    "            net,\n",
    "            (input_ids, attention_mask),  # у нас два тензора на вход\n",
    "            \"/root/onnx_models/roberta.onnx\",\n",
    "            verbose=False,\n",
    "            export_params=True,\n",
    "            opset_version=13,\n",
    "            do_constant_folding=True,\n",
    "            input_names=[\"input_ids\", \"attention_mask\"],\n",
    "            output_names=[\"logits\"],\n",
    "            dynamic_axes={\n",
    "                \"input_ids\": {0: \"batch\"},\n",
    "                \"attention_mask\": {0: \"batch\"},\n",
    "                \"logits\": {0: \"batch\"},\n",
    "            },\n",
    "            training=TrainingMode.EVAL,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34078c4e-7a72-4a27-8ba8-25816734ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer(\n",
    "    sequence_pairs,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=\"only_first\",\n",
    "    pad_to_multiple_of=128,\n",
    "    max_length=128,\n",
    ")\n",
    "\n",
    "save_as_onnx(model, x, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b05aaf-6f5b-4146-8835-985e7ef015c3",
   "metadata": {},
   "source": [
    "## Data for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa019276-ad76-492a-b279-74663154369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"/root/data/\"\n",
    "scores = pd.read_csv(data_folder + \"scores.csv\").values\n",
    "triton_scores = pd.read_csv(data_folder + \"triton_scores.csv\").values\n",
    "\n",
    "with open(data_folder + \"input.npy\", \"rb\") as f:\n",
    "    data_input = np.load(f)\n",
    "\n",
    "with open(data_folder + \"attention.npy\", \"rb\") as f:\n",
    "    data_attention = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b04d52-65a8-4693-91d0-efe7317b1593",
   "metadata": {},
   "source": [
    "### Test ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acbfca8f-b48c-4c2e-a4cd-811144722566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/bert_env/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:55: UserWarning: Specified provider 'TensorrtExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n",
      "  warnings.warn(\"Specified provider '{}' is not in available provider names.\"\n",
      "/root/bert_env/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:55: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n",
      "  warnings.warn(\"Specified provider '{}' is not in available provider names.\"\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "\n",
    "ort_sess = ort.InferenceSession(\n",
    "    \"/root/onnx_models/roberta.onnx\",\n",
    "    providers=[\n",
    "        \"TensorrtExecutionProvider\",\n",
    "        \"CUDAExecutionProvider\",\n",
    "        \"CPUExecutionProvider\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "711d9452-c55c-4425-9eaa-2ec6ab2f89ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_res = ort_sess.run(\n",
    "    None,\n",
    "    {\n",
    "        \"input_ids\": x[\"input_ids\"].numpy().astype(\"int32\"),\n",
    "        \"attention_mask\": x[\"attention_mask\"].numpy().astype(\"int32\"),\n",
    "    },\n",
    ")\n",
    "\n",
    "pre_output = onnx_res[0][:, [0, 2]]\n",
    "output = np.exp(pre_output) / np.sum(np.exp(pre_output), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b84ba9d-c0d5-4b2e-82ae-9c9aca92827c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 3, 128)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3092253d-be88-4468-b0f1-eb5c386c9da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 12 33 "
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    if (np.abs(triton_scores[i] - scores[i]) > 0.1).sum():\n",
    "        print(i, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2e5a55e-8c21-4436-9414-799aef5b8117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1790491, 0.0003463, 0.0007632])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(triton_scores[8] - scores[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cf14947-ab9c-41c3-8179-d71fb8271691",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_res = ort_sess.run(\n",
    "    None,\n",
    "    {\n",
    "        \"input_ids\": data_input[8].astype(\"int32\"),\n",
    "        \"attention_mask\": data_attention[8].astype(\"int32\"),\n",
    "    },\n",
    ")\n",
    "pre_output = onnx_res[0][:, [0, 2]]\n",
    "onnx_output = np.exp(pre_output) / np.sum(np.exp(pre_output), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "942f9976-58b8-4e81-b973-d47ba2bd4ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.09523384e-03, 2.29477386e-06, 4.38036804e-06])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(onnx_output[:, 1] - scores[8]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2c8f40d-8e3d-46d5-b25d-d0ed55169085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17695387, 0.00034401, 0.00075882])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(onnx_output[:, 1] - triton_scores[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aad3e166-b881-46f9-8dc2-1d5960d0aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_res = ort_sess.run(\n",
    "    None,\n",
    "    {\n",
    "        \"input_ids\": data_input[12].astype(\"int32\"),\n",
    "        \"attention_mask\": data_attention[12].astype(\"int32\"),\n",
    "    },\n",
    ")\n",
    "pre_output = onnx_res[0][:, [0, 2]]\n",
    "onnx_output = np.exp(pre_output) / np.sum(np.exp(pre_output), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ef5f699-cd14-4826-8574-f0da170532ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.43510107e-04, 2.85875992e-06, 2.85875992e-06])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(onnx_output[:, 1] - scores[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c519c1-4b0f-45ca-9586-d1cc3852262d",
   "metadata": {},
   "source": [
    "### TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cb28481-a048-445d-be04-eae2f618ce32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8003] # /usr/src/tensorrt/bin/trtexec --onnx=/root/onnx_models/roberta.onnx --saveEngine=/root/trt_models/roberta.trt --workspace=80000 --minShapes=input_ids:1x128,attention_mask:1x128 --optShapes=input_ids:3x128,attention_mask:3x128 --maxShapes=input_ids:10x128,attention_mask:10x128\n",
      "[05/29/2022-20:23:07] [I] === Model Options ===\n",
      "[05/29/2022-20:23:07] [I] Format: ONNX\n",
      "[05/29/2022-20:23:07] [I] Model: /root/onnx_models/roberta.onnx\n",
      "[05/29/2022-20:23:07] [I] Output:\n",
      "[05/29/2022-20:23:07] [I] === Build Options ===\n",
      "[05/29/2022-20:23:07] [I] Max batch: explicit\n",
      "[05/29/2022-20:23:07] [I] Workspace: 80000 MiB\n",
      "[05/29/2022-20:23:07] [I] minTiming: 1\n",
      "[05/29/2022-20:23:07] [I] avgTiming: 8\n",
      "[05/29/2022-20:23:07] [I] Precision: FP32\n",
      "[05/29/2022-20:23:07] [I] Calibration: \n",
      "[05/29/2022-20:23:07] [I] Refit: Disabled\n",
      "[05/29/2022-20:23:07] [I] Sparsity: Disabled\n",
      "[05/29/2022-20:23:07] [I] Safe mode: Disabled\n",
      "[05/29/2022-20:23:07] [I] Restricted mode: Disabled\n",
      "[05/29/2022-20:23:07] [I] Save engine: /root/trt_models/roberta.trt\n",
      "[05/29/2022-20:23:07] [I] Load engine: \n",
      "[05/29/2022-20:23:07] [I] NVTX verbosity: 0\n",
      "[05/29/2022-20:23:07] [I] Tactic sources: Using default tactic sources\n",
      "[05/29/2022-20:23:07] [I] timingCacheMode: local\n",
      "[05/29/2022-20:23:07] [I] timingCacheFile: \n",
      "[05/29/2022-20:23:07] [I] Input(s)s format: fp32:CHW\n",
      "[05/29/2022-20:23:07] [I] Output(s)s format: fp32:CHW\n",
      "[05/29/2022-20:23:07] [I] Input build shape: attention_mask=1x128+3x128+10x128\n",
      "[05/29/2022-20:23:07] [I] Input build shape: input_ids=1x128+3x128+10x128\n",
      "[05/29/2022-20:23:07] [I] Input calibration shapes: model\n",
      "[05/29/2022-20:23:07] [I] === System Options ===\n",
      "[05/29/2022-20:23:07] [I] Device: 0\n",
      "[05/29/2022-20:23:07] [I] DLACore: \n",
      "[05/29/2022-20:23:07] [I] Plugins:\n",
      "[05/29/2022-20:23:07] [I] === Inference Options ===\n",
      "[05/29/2022-20:23:07] [I] Batch: Explicit\n",
      "[05/29/2022-20:23:07] [I] Input inference shape: input_ids=3x128\n",
      "[05/29/2022-20:23:07] [I] Input inference shape: attention_mask=3x128\n",
      "[05/29/2022-20:23:07] [I] Iterations: 10\n",
      "[05/29/2022-20:23:07] [I] Duration: 3s (+ 200ms warm up)\n",
      "[05/29/2022-20:23:07] [I] Sleep time: 0ms\n",
      "[05/29/2022-20:23:07] [I] Streams: 1\n",
      "[05/29/2022-20:23:07] [I] ExposeDMA: Disabled\n",
      "[05/29/2022-20:23:07] [I] Data transfers: Enabled\n",
      "[05/29/2022-20:23:07] [I] Spin-wait: Disabled\n",
      "[05/29/2022-20:23:07] [I] Multithreading: Disabled\n",
      "[05/29/2022-20:23:07] [I] CUDA Graph: Disabled\n",
      "[05/29/2022-20:23:07] [I] Separate profiling: Disabled\n",
      "[05/29/2022-20:23:07] [I] Time Deserialize: Disabled\n",
      "[05/29/2022-20:23:07] [I] Time Refit: Disabled\n",
      "[05/29/2022-20:23:07] [I] Skip inference: Disabled\n",
      "[05/29/2022-20:23:07] [I] Inputs:\n",
      "[05/29/2022-20:23:07] [I] === Reporting Options ===\n",
      "[05/29/2022-20:23:07] [I] Verbose: Disabled\n",
      "[05/29/2022-20:23:07] [I] Averages: 10 inferences\n",
      "[05/29/2022-20:23:07] [I] Percentile: 99\n",
      "[05/29/2022-20:23:07] [I] Dump refittable layers:Disabled\n",
      "[05/29/2022-20:23:07] [I] Dump output: Disabled\n",
      "[05/29/2022-20:23:07] [I] Profile: Disabled\n",
      "[05/29/2022-20:23:07] [I] Export timing to JSON file: \n",
      "[05/29/2022-20:23:07] [I] Export output to JSON file: \n",
      "[05/29/2022-20:23:07] [I] Export profile to JSON file: \n",
      "[05/29/2022-20:23:07] [I] \n",
      "[05/29/2022-20:23:07] [I] === Device Information ===\n",
      "[05/29/2022-20:23:07] [I] Selected Device: NVIDIA A100-PCIE-40GB\n",
      "[05/29/2022-20:23:07] [I] Compute Capability: 8.0\n",
      "[05/29/2022-20:23:07] [I] SMs: 108\n",
      "[05/29/2022-20:23:07] [I] Compute Clock Rate: 1.41 GHz\n",
      "[05/29/2022-20:23:07] [I] Device Global Memory: 40536 MiB\n",
      "[05/29/2022-20:23:07] [I] Shared Memory per SM: 164 KiB\n",
      "[05/29/2022-20:23:07] [I] Memory Bus Width: 5120 bits (ECC enabled)\n",
      "[05/29/2022-20:23:07] [I] Memory Clock Rate: 1.215 GHz\n",
      "[05/29/2022-20:23:07] [I] \n",
      "[05/29/2022-20:23:07] [I] TensorRT version: 8003\n",
      "[05/29/2022-20:23:07] [I] [TRT] [MemUsageChange] Init CUDA: CPU +499, GPU +0, now: CPU 506, GPU 8105 (MiB)\n",
      "[05/29/2022-20:23:07] [I] Start parsing network model\n",
      "[05/29/2022-20:23:07] [I] [TRT] ----------------------------------------------------------------\n",
      "[05/29/2022-20:23:07] [I] [TRT] Input filename:   /root/onnx_models/roberta.onnx\n",
      "[05/29/2022-20:23:07] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[05/29/2022-20:23:07] [I] [TRT] Opset version:    13\n",
      "[05/29/2022-20:23:07] [I] [TRT] Producer name:    pytorch\n",
      "[05/29/2022-20:23:07] [I] [TRT] Producer version: 1.11.0\n",
      "[05/29/2022-20:23:07] [I] [TRT] Domain:           \n",
      "[05/29/2022-20:23:07] [I] [TRT] Model version:    0\n",
      "[05/29/2022-20:23:07] [I] [TRT] Doc string:       \n",
      "[05/29/2022-20:23:07] [I] [TRT] ----------------------------------------------------------------\n",
      "[05/29/2022-20:23:09] [W] [TRT] onnx2trt_utils.cpp:362: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[05/29/2022-20:23:14] [W] [TRT] Output type must be INT32 for shape outputs\n",
      "[05/29/2022-20:23:14] [I] Finish parsing network model\n",
      "[05/29/2022-20:23:14] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 2652, GPU 8105 (MiB)\n",
      "[05/29/2022-20:23:14] [I] [TRT] [MemUsageSnapshot] Builder begin: CPU 2652 MiB, GPU 8105 MiB\n",
      "[05/29/2022-20:23:17] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +792, GPU +340, now: CPU 3446, GPU 8445 (MiB)\n",
      "[05/29/2022-20:23:18] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +195, GPU +344, now: CPU 3641, GPU 8789 (MiB)\n",
      "[05/29/2022-20:23:18] [W] [TRT] Detected invalid timing cache, setup a local cache instead\n",
      "[05/29/2022-20:23:25] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[05/29/2022-20:23:25] [W] [TRT]  (# 0 (SHAPE attention_mask))\n",
      "[05/29/2022-20:23:25] [W] [TRT]  (# 0 (SHAPE input_ids))\n",
      "[05/29/2022-20:24:39] [I] [TRT] Detected 2 inputs and 1 output network tensors.\n",
      "[05/29/2022-20:24:46] [W] [TRT] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are: \n",
      "[05/29/2022-20:24:46] [W] [TRT]  (# 0 (SHAPE attention_mask))\n",
      "[05/29/2022-20:24:46] [W] [TRT]  (# 0 (SHAPE input_ids))\n",
      "[05/29/2022-20:25:50] [I] [TRT] Total Host Persistent Memory: 5568\n",
      "[05/29/2022-20:25:50] [I] [TRT] Total Device Persistent Memory: 4219392\n",
      "[05/29/2022-20:25:50] [I] [TRT] Total Scratch Memory: 62921296\n",
      "[05/29/2022-20:25:50] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 4 MiB, GPU 4 MiB\n",
      "[05/29/2022-20:25:50] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 4353, GPU 12437 (MiB)\n",
      "[05/29/2022-20:25:50] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 4353, GPU 12445 (MiB)\n",
      "[05/29/2022-20:25:50] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 4353, GPU 12429 (MiB)\n",
      "[05/29/2022-20:25:50] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 4353, GPU 12411 (MiB)\n",
      "[05/29/2022-20:25:50] [I] [TRT] [MemUsageSnapshot] Builder end: CPU 4351 MiB, GPU 12411 MiB\n",
      "[05/29/2022-20:26:01] [I] [TRT] Loaded engine size: 3342 MB\n",
      "[05/29/2022-20:26:01] [I] [TRT] [MemUsageSnapshot] deserializeCudaEngine begin: CPU 7682 MiB, GPU 9117 MiB\n",
      "[05/29/2022-20:26:10] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 7693, GPU 12417 (MiB)\n",
      "[05/29/2022-20:26:10] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 7693, GPU 12425 (MiB)\n",
      "[05/29/2022-20:26:10] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 7693, GPU 12407 (MiB)\n",
      "[05/29/2022-20:26:10] [I] [TRT] [MemUsageSnapshot] deserializeCudaEngine end: CPU 7693 MiB, GPU 12407 MiB\n",
      "[05/29/2022-20:26:25] [I] Engine built in 198.078 sec.\n",
      "[05/29/2022-20:26:25] [I] [TRT] [MemUsageSnapshot] ExecutionContext creation begin: CPU 2205 MiB, GPU 12407 MiB\n",
      "[05/29/2022-20:26:25] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2205, GPU 12417 (MiB)\n",
      "[05/29/2022-20:26:25] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2205, GPU 12425 (MiB)\n",
      "[05/29/2022-20:26:27] [I] [TRT] [MemUsageSnapshot] ExecutionContext creation end: CPU 2273 MiB, GPU 12521 MiB\n",
      "[05/29/2022-20:26:27] [I] Created input binding for input_ids with dimensions 3x128\n",
      "[05/29/2022-20:26:27] [I] Created input binding for attention_mask with dimensions 3x128\n",
      "[05/29/2022-20:26:27] [I] Created output binding for logits with dimensions 3x3\n",
      "[05/29/2022-20:26:27] [I] Starting inference\n",
      "[05/29/2022-20:26:30] [I] Warmup completed 20 queries over 200 ms\n",
      "[05/29/2022-20:26:30] [I] Timing trace has 494 queries over 3.00804 s\n",
      "[05/29/2022-20:26:30] [I] \n",
      "[05/29/2022-20:26:30] [I] === Trace details ===\n",
      "[05/29/2022-20:26:30] [I] Trace averages of 10 runs:\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.1376 ms - Host latency: 6.16204 ms (end to end 6.1777 ms, enqueue 6.11792 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.20811 ms - Host latency: 6.23805 ms (end to end 6.25427 ms, enqueue 6.18921 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 5.97713 ms - Host latency: 6.00224 ms (end to end 6.01847 ms, enqueue 5.95806 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.04959 ms - Host latency: 6.07514 ms (end to end 6.09239 ms, enqueue 6.03116 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.06159 ms - Host latency: 6.08714 ms (end to end 6.10382 ms, enqueue 6.04158 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.05377 ms - Host latency: 6.07818 ms (end to end 6.09331 ms, enqueue 6.03184 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.04915 ms - Host latency: 6.07408 ms (end to end 6.08987 ms, enqueue 6.02892 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.04028 ms - Host latency: 6.06487 ms (end to end 6.07983 ms, enqueue 6.0195 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.03553 ms - Host latency: 6.05996 ms (end to end 6.07539 ms, enqueue 6.01448 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.0437 ms - Host latency: 6.06851 ms (end to end 6.0843 ms, enqueue 6.02274 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.04287 ms - Host latency: 6.06805 ms (end to end 6.08483 ms, enqueue 6.02437 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.04305 ms - Host latency: 6.06841 ms (end to end 6.08552 ms, enqueue 6.02343 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.04581 ms - Host latency: 6.07065 ms (end to end 6.08809 ms, enqueue 6.02622 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.04315 ms - Host latency: 6.06799 ms (end to end 6.08477 ms, enqueue 6.02563 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.03646 ms - Host latency: 6.06118 ms (end to end 6.07638 ms, enqueue 6.01534 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.03214 ms - Host latency: 6.05737 ms (end to end 6.07369 ms, enqueue 6.01221 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.46718 ms - Host latency: 6.50668 ms (end to end 6.52258 ms, enqueue 6.44827 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 5.98484 ms - Host latency: 6.01082 ms (end to end 6.02852 ms, enqueue 5.9678 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.03417 ms - Host latency: 6.05905 ms (end to end 6.07565 ms, enqueue 6.01447 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.04069 ms - Host latency: 6.06578 ms (end to end 6.08164 ms, enqueue 6.02094 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.0442 ms - Host latency: 6.06898 ms (end to end 6.08513 ms, enqueue 6.02371 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.04752 ms - Host latency: 6.07218 ms (end to end 6.08781 ms, enqueue 6.02695 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.04862 ms - Host latency: 6.07482 ms (end to end 6.09189 ms, enqueue 6.03076 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.05219 ms - Host latency: 6.07726 ms (end to end 6.0934 ms, enqueue 6.03197 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.05051 ms - Host latency: 6.07618 ms (end to end 6.09408 ms, enqueue 6.03257 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.05342 ms - Host latency: 6.07836 ms (end to end 6.09447 ms, enqueue 6.03329 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.05094 ms - Host latency: 6.07583 ms (end to end 6.091 ms, enqueue 6.03046 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.051 ms - Host latency: 6.07556 ms (end to end 6.09147 ms, enqueue 6.03214 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.05394 ms - Host latency: 6.07881 ms (end to end 6.09418 ms, enqueue 6.03395 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.05874 ms - Host latency: 6.08362 ms (end to end 6.10013 ms, enqueue 6.03939 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.045 ms - Host latency: 6.06968 ms (end to end 6.08533 ms, enqueue 6.02352 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.04873 ms - Host latency: 6.07356 ms (end to end 6.08933 ms, enqueue 6.02773 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.0509 ms - Host latency: 6.07534 ms (end to end 6.08987 ms, enqueue 6.03162 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.06887 ms - Host latency: 6.0959 ms (end to end 6.11162 ms, enqueue 6.05078 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.25303 ms - Host latency: 6.27874 ms (end to end 6.29451 ms, enqueue 6.23535 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.04551 ms - Host latency: 6.07053 ms (end to end 6.08582 ms, enqueue 6.02563 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.04419 ms - Host latency: 6.06895 ms (end to end 6.08394 ms, enqueue 6.02361 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.04539 ms - Host latency: 6.06946 ms (end to end 6.08228 ms, enqueue 6.02253 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.04641 ms - Host latency: 6.0709 ms (end to end 6.08591 ms, enqueue 6.02556 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.05317 ms - Host latency: 6.07917 ms (end to end 6.09416 ms, enqueue 6.02991 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.04988 ms - Host latency: 6.07495 ms (end to end 6.09097 ms, enqueue 6.03054 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.04614 ms - Host latency: 6.07036 ms (end to end 6.08472 ms, enqueue 6.02466 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.04551 ms - Host latency: 6.06973 ms (end to end 6.08408 ms, enqueue 6.02356 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.04578 ms - Host latency: 6.06968 ms (end to end 6.08396 ms, enqueue 6.02427 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.03975 ms - Host latency: 6.06411 ms (end to end 6.07781 ms, enqueue 6.01663 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.03938 ms - Host latency: 6.06455 ms (end to end 6.0804 ms, enqueue 6.01975 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.04263 ms - Host latency: 6.06658 ms (end to end 6.08052 ms, enqueue 6.02202 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.04854 ms - Host latency: 6.07231 ms (end to end 6.08735 ms, enqueue 6.0271 ms)\n",
      "[05/29/2022-20:26:30] [I] Average on 10 runs - GPU latency: 6.11082 ms - Host latency: 6.13477 ms (end to end 6.15098 ms, enqueue 6.09231 ms)\n",
      "[05/29/2022-20:26:30] [I] \n",
      "[05/29/2022-20:26:30] [I] === Performance summary ===\n",
      "[05/29/2022-20:26:30] [I] Throughput: 164.226 qps\n",
      "[05/29/2022-20:26:30] [I] Latency: min = 5.97194 ms, max = 7.46545 ms, mean = 6.09367 ms, median = 6.0704 ms, percentile(99%) = 6.92212 ms\n",
      "[05/29/2022-20:26:30] [I] End-to-End Host Latency: min = 5.98622 ms, max = 7.47571 ms, mean = 6.10941 ms, median = 6.0853 ms, percentile(99%) = 6.94049 ms\n",
      "[05/29/2022-20:26:30] [I] Enqueue Time: min = 5.92651 ms, max = 7.41736 ms, mean = 6.04822 ms, median = 6.02484 ms, percentile(99%) = 6.83435 ms\n",
      "[05/29/2022-20:26:30] [I] H2D Latency: min = 0.0107422 ms, max = 0.0760498 ms, mean = 0.0134614 ms, median = 0.0130005 ms, percentile(99%) = 0.0244141 ms\n",
      "[05/29/2022-20:26:30] [I] GPU Compute Time: min = 5.94907 ms, max = 7.44324 ms, mean = 6.06823 ms, median = 6.04614 ms, percentile(99%) = 6.86703 ms\n",
      "[05/29/2022-20:26:30] [I] D2H Latency: min = 0.0065918 ms, max = 0.067749 ms, mean = 0.0119852 ms, median = 0.0117188 ms, percentile(99%) = 0.0244141 ms\n",
      "[05/29/2022-20:26:30] [I] Total Host Walltime: 3.00804 s\n",
      "[05/29/2022-20:26:30] [I] Total GPU Compute Time: 2.99771 s\n",
      "[05/29/2022-20:26:30] [W] * Throughput may be bound by Enqueue Time rather than GPU Compute and the GPU may be under-utilized.\n",
      "[05/29/2022-20:26:30] [W]   If not already in use, --useCudaGraph (utilize CUDA graphs where possible) may increase the throughput.\n",
      "[05/29/2022-20:26:30] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[05/29/2022-20:26:30] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8003] # /usr/src/tensorrt/bin/trtexec --onnx=/root/onnx_models/roberta.onnx --saveEngine=/root/trt_models/roberta.trt --workspace=80000 --minShapes=input_ids:1x128,attention_mask:1x128 --optShapes=input_ids:3x128,attention_mask:3x128 --maxShapes=input_ids:10x128,attention_mask:10x128\n",
      "[05/29/2022-20:26:30] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 2205, GPU 12491 (MiB)\n"
     ]
    }
   ],
   "source": [
    "!/usr/src/tensorrt/bin/trtexec --onnx=/root/onnx_models/roberta.onnx --saveEngine=/root/trt_models/roberta.trt --workspace=80000 --minShapes=input_ids:1x128,attention_mask:1x128 --optShapes=input_ids:3x128,attention_mask:3x128 --maxShapes=input_ids:10x128,attention_mask:10x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec41ce4-4af2-4f25-828e-3b1eb5ecea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /root/trt_models/roberta.trt /root/trt_models/model.plan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERT kernel",
   "language": "python",
   "name": "bert_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
